{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import codecs\n",
    "import unidecode\n",
    "import string\n",
    "import copy\n",
    "import random\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "\n",
    "# Eval metric\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "chencherry = SmoothingFunction()\n",
    "\n",
    "# reproduce !\n",
    "random.seed(11)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "filename = \"/Users/pkabara/translation/deu.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_of_sentences(filename):\n",
    "    pairs = []\n",
    "    with codecs.open(filename,'r',encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # lower case\n",
    "            line = line.lower()\n",
    "            # remove accents \"ą/ę/ś/ć\" with \"a/e/s/c\"\n",
    "            line = unidecode.unidecode(line)\n",
    "            # remove punctuations\n",
    "            line = line.translate(translator)\n",
    "            #remove digit\n",
    "            line = ''.join(i for i in line if not i.isdigit())\n",
    "            \n",
    "            sentence, conversion = line.split(\"\\t\") \n",
    "            pairs.append([sentence,conversion])\n",
    "            \n",
    "    return pairs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self):\n",
    "        self.SOS = \"<SOS>\" # Start of Sentence\n",
    "        self.EOS = \"<EOS>\" # End of Sentence\n",
    "        self.counter = 3\n",
    "        self.word2index = {\"<PAD>\":0,\"<SOS>\":1, \"<EOS>\":2}\n",
    "        self.index2word = {0:\"<PAD>\",1:\"<SOS>\", 2:\"<EOS>\"}\n",
    "        self.word2count = {}\n",
    "    \n",
    "    def add_word(self,word):\n",
    "        if word in self.word2index:\n",
    "            self.word2count[word] += 1\n",
    "        else:\n",
    "            self.word2index[word] = self.counter\n",
    "            self.index2word[self.counter] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.counter += 1\n",
    "    \n",
    "    def add_sentense(self,words):\n",
    "        for each in words:\n",
    "            self.add_word(each)\n",
    "    \n",
    "    def filter_words(self,threshold):\n",
    "        counter = 3\n",
    "        word2index = {\"<PAD>\":0,\"<SOS>\":1, \"<EOS>\":2}\n",
    "        index2word = {0:\"<PAD>\",1:\"<SOS>\", 2:\"<EOS>\"}\n",
    "        word2count = {}\n",
    "        tlen = len(self.word2index)\n",
    "        for each in self.word2count:\n",
    "            count = self.word2count[each]\n",
    "            if count >= threshold:\n",
    "                word2index[each] = counter\n",
    "                index2word[counter] = each\n",
    "                word2count[each] = self.word2count[each]\n",
    "                counter += 1\n",
    "        self.word2index = word2index\n",
    "        self.index2word = index2word\n",
    "        self.word2count = word2count\n",
    "        self.counter = counter\n",
    "        print (\"words keep ratio {}\".format(len(self.word2index)/tlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_all_word_present(list_words,language):\n",
    "    is_include = True\n",
    "    for word in list_words:\n",
    "        if word not in language.word2index:\n",
    "            is_include = False\n",
    "            break\n",
    "    return is_include\n",
    "\n",
    "def filter_sentences(splitted_sentence_pair,language1,language2,max_len=20,min_len=3):\n",
    "    new_list = []\n",
    "    for each in splitted_sentence_pair:\n",
    "        is_include = (min_len <= len(each[0]) <= max_len) and (min_len <= len(each[1]) <= max_len)\n",
    "        is_include = is_include and is_all_word_present(each[0],language1) and is_all_word_present(each[1],language2)\n",
    "        if is_include:\n",
    "            new_list.append([each[0],each[1]])\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words keep ratio 0.5471966852259484\n",
      "words keep ratio 0.4013365509560052\n",
      "before 176692 after 149894 ratio 0.8483349557421954\n"
     ]
    }
   ],
   "source": [
    "sentence_pair = pair_of_sentences(filename)\n",
    "splitted_sentence_pair = []\n",
    "for each in sentence_pair:\n",
    "    splitted_sentence_pair.append((each[0].split(\" \"), each[1].split(\" \")))\n",
    "\n",
    "del sentence_pair\n",
    "\n",
    "english = Language()\n",
    "german = Language()\n",
    "\n",
    "for each in splitted_sentence_pair:\n",
    "    english.add_sentense(each[0])\n",
    "    german.add_sentense(each[1])\n",
    "    \n",
    "english.filter_words(3)\n",
    "german.filter_words(3)\n",
    "\n",
    "tlen = len(splitted_sentence_pair)\n",
    "splitted_sentence_pair = filter_sentences(splitted_sentence_pair,english,german)\n",
    "tlen2 = len(splitted_sentence_pair)\n",
    "print (\"before {} after {} ratio {}\".format(tlen,tlen2,tlen2/tlen))\n",
    "\n",
    "# shuffle !\n",
    "shuffle(splitted_sentence_pair)\n",
    "\n",
    "number_val = 50\n",
    "val_splitted_sentence_pair = splitted_sentence_pair[-number_val:]\n",
    "train_splitted_sentence_pair  = splitted_sentence_pair[:-number_val]\n",
    "del splitted_sentence_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,sentence_pairs,src_lang,target_lang,batch_size=32):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.src_lang = src_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.len = len(self.sentence_pairs)\n",
    "        self.c = 0 \n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def add_eos(self,pairs):\n",
    "        for i in range(len(pairs)):\n",
    "            pairs[i][0].append(\"<EOS>\")\n",
    "            pairs[i][1].append(\"<EOS>\")\n",
    "    \n",
    "    def get_max_len(self,pairs):\n",
    "        m1, m2 = 0, 0\n",
    "        src, tar = [], []\n",
    "        for each in pairs:\n",
    "            l1 = len(each[0])\n",
    "            l2 = len(each[1])\n",
    "            m1 = max(m1,l1)\n",
    "            m2 = max(m2,l2)\n",
    "            src.append(l1)\n",
    "            tar.append(l2)\n",
    "        return m1,m2,src,tar\n",
    "    \n",
    "    def pad_src_target(self,pairs,src_max,target_max):\n",
    "        for i in range(len(pairs)):\n",
    "            pairs[i][0] += [\"<PAD>\",] * (src_max - len(pairs[i][0]))\n",
    "            pairs[i][1] += [\"<PAD>\",] * (target_max - len(pairs[i][1]))\n",
    "    \n",
    "    def covert_sentence_index(self,sentence,language):\n",
    "        words = []\n",
    "        for each in sentence:\n",
    "            words.append(language.word2index[each])\n",
    "        return words\n",
    "    \n",
    "    def convert_to_index(self,pairs):\n",
    "        new_pairs = []\n",
    "        for each in pairs:\n",
    "            indexed_sen_src = self.covert_sentence_index(each[0],self.src_lang)\n",
    "            indexed_sen_tar = self.covert_sentence_index(each[1],self.target_lang)\n",
    "            new_pairs.append([indexed_sen_src,indexed_sen_tar])\n",
    "        return new_pairs\n",
    "            \n",
    "    def get_batch(self):\n",
    "        pairs = []\n",
    "        for i in range(self.batch_size):\n",
    "            if self.c >= self.len:\n",
    "                shuffle(self.sentence_pairs)\n",
    "                self.c = 0\n",
    "            pairs.append(self.sentence_pairs[self.c])\n",
    "            self.c += 1\n",
    "        \n",
    "        # don't want to affect original pairs ... \n",
    "        # make a deepcopy \n",
    "        pairs = copy.deepcopy(pairs)\n",
    "        \n",
    "        # add end of sentence tag !\n",
    "        self.add_eos(pairs)\n",
    "        # sort by reverse as torch nn pack_padded_sequence required !\n",
    "        pairs.sort(key=lambda p: len(p[0]), reverse=True)\n",
    "        \n",
    "        m1, m2, src_lens, tar_lens = self.get_max_len(pairs)\n",
    "        \n",
    "        # pad source language with m1 and target language with m2\n",
    "        self.pad_src_target(pairs,m1,m2)\n",
    "        \n",
    "        # convert to index \n",
    "        pairs = self.convert_to_index(pairs)\n",
    "        \n",
    "        src, tar = zip(*pairs)\n",
    "        \n",
    "        # batch * timesteps * word\n",
    "        return src, tar, src_lens, tar_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers=2,dropout=0.5):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size)\n",
    "        self.gru = nn.GRU(input_size=hidden_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,dropout=dropout,\n",
    "                          bidirectional=True)\n",
    "    \n",
    "    def forward(self,input_data,input_len,hidden_data=None):\n",
    "        \"\"\"\n",
    "        # time x batch x words (T x B x W)\n",
    "        \"\"\"\n",
    "        input_embedded = self.embedding(input_data) # T x B x W x Embed_Size\n",
    "        input_embedded = torch.nn.utils.rnn.pack_padded_sequence(input_embedded,input_len) # T x B x W x Embed_Size\n",
    "        output, hidden = self.gru(input_embedded,hidden_data)\n",
    "        output, output_len = torch.nn.utils.rnn.pad_packed_sequence(output)\n",
    "        # output_len will alwatys be same as input_len !\n",
    "        # add forward pass output and backward pass output (Bidirectional RNN) \n",
    "        output = output[:,:,:self.hidden_size] + output[:,:,self.hidden_size:] # T x B x Hidden_Size\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self,score_type,hidden_size):\n",
    "        super(Attn,self).__init__()\n",
    "        if score_type not in ('dot','concat','general'):\n",
    "            raise ValueError('score should be one of the in dot, concat, general !')\n",
    "        \n",
    "        self.score_type = score_type\n",
    "        self.hidden_size = hidden_size \n",
    "        \n",
    "        if score_type=='general':\n",
    "            self.w_a = torch.nn.Linear(hidden_size,hidden_size)\n",
    "        if score_type=='concat':\n",
    "            self.w_a = torch.nn.Linear(2*hidden_size,hidden_size)\n",
    "            self.v_a = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "        \n",
    "    def forward(self,encoder_outputs,hidden_state):\n",
    "        \"\"\"\n",
    "        encoder_outputs # T x B x Hidden_Size\n",
    "        hidden_state # B x Hidden_Size\n",
    "        \"\"\"\n",
    "        hidden_state = hidden_state.unsqueeze(0) # 1 x B x Hidden_Size\n",
    "        activation = None\n",
    "        if self.score_type=='dot':\n",
    "            dot_prod = hidden_state * encoder_outputs # T x B x Hidden_Size\n",
    "            dot_prod = torch.sum(dot_prod,dim=2) # T x B \n",
    "            activation = torch.softmax(dot_prod,dim=0) # T x B (across time softmax)\n",
    "        if self.score_type=='general':\n",
    "            # only 1st step is different than dot !\n",
    "            dot_prod = self.w_a(encoder_outputs) # T x B x Hidden_Size \n",
    "            dot_prod = hidden_state * encoder_outputs # T x B x Hidden_Size\n",
    "            dot_prod = torch.sum(dot_prod,dim=2) # T x B \n",
    "            activation = torch.softmax(dot_prod,dim=0) # T x B (across time softmax)\n",
    "        if self.score_type=='concat':\n",
    "            # concat !\n",
    "            T = encoder_outputs.shape[0]\n",
    "            hidden_state = hidden_state.repeat((T,1,1)) # T x B x Hidden_Size\n",
    "            # one decoder step concatenated to all encoder states\n",
    "            concat_tenc_1dec = torch.cat((encoder_outputs,hidden_state),dim=2)  # T x B x (2*Hidden_Size)\n",
    "            concat_tenc_1dec = torch.tanh(self.w_a(concat_tenc_1dec)) # T x B x Hidden_Size\n",
    "            dot_prod = self.v_a * concat_tenc_1dec # T x B x Hidden_Size\n",
    "            dot_prod = torch.sum(dot_prod,dim=2) # T x B \n",
    "            activation = torch.softmax(dot_prod,dim=0) # T x B (across time softmax)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers=2,dropout=0.5,score_type='concat'):\n",
    "        \"\"\"\n",
    "        input_size => number of words in target language\n",
    "        \"\"\"\n",
    "        super(Decoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size)\n",
    "        self.gru = nn.GRU(input_size=hidden_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,dropout=dropout,\n",
    "                          bidirectional=False)\n",
    "        self.attn = Attn(score_type,hidden_size)\n",
    "        self.w_c = torch.nn.Linear(2*hidden_size,hidden_size)\n",
    "        self.w_s = torch.nn.Linear(hidden_size,input_size)\n",
    "        \n",
    "    def forward(self,previous_hidden,input_data,encoder_outputs):\n",
    "        \"\"\"\n",
    "        input_data => B x Word\n",
    "        previous_hidden => num_layers x Batch x hidden_size\n",
    "        encoder_outputs => T x B x Hidden_Size\n",
    "        \"\"\"\n",
    "        embedded_input = self.embedding(input_data) # B x Word x embedding_size\n",
    "        dummy_time_embedded_input = embedded_input.unsqueeze(0) # 1 x B x Word x embedding_size\n",
    "        output, hidden = self.gru(dummy_time_embedded_input,previous_hidden)\n",
    "        # output # 1 x B x hidden_size\n",
    "        output = output.squeeze(0) # B x hidden_size\n",
    "        attention = self.attn.forward(encoder_outputs,output) # T x B\n",
    "        attention = attention.unsqueeze(2) # T x B X 1\n",
    "        weighted_context = encoder_outputs * attention # T x B x Hidden_Size\n",
    "        weighted_context = torch.sum(weighted_context,dim=0) # B x Hidden_Size\n",
    "        contexted_output = torch.cat((weighted_context,output),dim=1) # B x (2*Hidden_Size)\n",
    "        contexted_output = torch.tanh ( self.w_c(contexted_output) ) # equation 5 # B x Hidden_Size\n",
    "        contexted_output = torch.softmax ( self.w_s(contexted_output) ,dim=1) # equation 6 # B x input_size\n",
    "        return contexted_output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(target,prediction,mask):\n",
    "    \"\"\"\n",
    "    # target => Batch\n",
    "    # prediction => Batch * Words\n",
    "    # mask => Batch\n",
    "    \"\"\"\n",
    "    non_zero_ele = mask.sum()\n",
    "    pred_at_y = torch.gather(prediction,dim=1,index=target.view((-1,1)))\n",
    "    loss = -torch.log(pred_at_y)\n",
    "    masked_selected_loss = loss.masked_select(mask.view((-1,1)))\n",
    "    total_avgloss = masked_selected_loss.mean()\n",
    "    return total_avgloss, non_zero_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a_batch(encoder,decoder,src,src_lens,tar,tar_lens,\n",
    "                  max_tar_len,\n",
    "                  encoder_optimizer,decoder_optimizer,\n",
    "                  num_layers, num_directions, batch, hidden_size,\n",
    "                  teacher_forcing_ratio=0.75,clip=50.0):\n",
    "        \"\"\"\n",
    "        src = B x T x word_indexes\n",
    "        tar = B x T x word_indexes\n",
    "        \"\"\"\n",
    "        # train mode\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        # clean out gradients\n",
    "        encoder_optimizer.zero_grad()   # zero the gradient buffers\n",
    "        decoder_optimizer.zero_grad()   # zero the gradient buffers\n",
    "        \n",
    "        # convert input to tensors !\n",
    "        src = torch.LongTensor(src)\n",
    "        src = src.transpose(1,0) #  T x B x words\n",
    "        src_lens = torch.LongTensor(src_lens) # B\n",
    "        tar = torch.LongTensor(tar)\n",
    "        tar = tar.transpose(1,0) #  T x B x words\n",
    "        tar_lens = torch.LongTensor(tar_lens) # B\n",
    "        \n",
    "        src = src.to(device)\n",
    "        src_lens = src_lens.to(device)\n",
    "        tar = tar.to(device)\n",
    "        tar_lens = tar_lens.to(device)\n",
    "        \n",
    "        # pass inputs from encoder\n",
    "        enc_output, enc_hidden = encoder.forward(src,src_lens)\n",
    "        enc_hidden = enc_hidden.view(num_layers, num_directions, batch, hidden_size)\n",
    "        enc_forward_hidden_layer = enc_hidden[:,0] # out of bidirectional GRU\n",
    "        # try to comment below line check what happens ! (Nothing happen on CPU but GPU get below error)\n",
    "        # it throws and exception ... after googling the exception (rnn: hx is not contiguous)\n",
    "        # https://stackoverflow.com/questions/48915810/pytorch-contiguous\n",
    "        enc_forward_hidden_layer = enc_forward_hidden_layer.contiguous()\n",
    "        \n",
    "        # decoder intial input # <SOS> start of sentence (its mapped to one in our case)\n",
    "        dec_input = torch.ones((batch)).long() # B \n",
    "        dec_input = dec_input.to(device)\n",
    "        is_teacher_force = random.uniform(0, 1) < teacher_forcing_ratio\n",
    "        \n",
    "        # loop over from decoder\n",
    "        total_avgloss, total_loss, total_count = 0.0, 0.0, 0.0\n",
    "        if is_teacher_force:\n",
    "            # feed actual output\n",
    "            dec_hidden = enc_forward_hidden_layer\n",
    "            for i in range(max_tar_len):\n",
    "                curr_target = tar[i]\n",
    "                curr_mask = i < tar_lens\n",
    "                if i!=0:\n",
    "                    # fisrt input will be <SOS>\n",
    "                    # i-1 because first word will be feeded to second decoder (check the seq2seq fig. online)\n",
    "                    dec_input = tar[i-1]\n",
    "                # dec_hidden assign to self to for next step\n",
    "                dec_pred, dec_hidden = decoder.forward(dec_hidden, dec_input, enc_output)\n",
    "                # dec_pred # B x vocab_size_of_translated_lang\n",
    "                curr_avgloss, curr_non_zero = loss(curr_target,dec_pred,curr_mask)\n",
    "                total_avgloss += curr_avgloss\n",
    "                total_count += curr_non_zero.item()\n",
    "                total_loss += curr_avgloss.item() * curr_non_zero.item() \n",
    "        else:\n",
    "            # feed predicted output by decoder\n",
    "            dec_hidden = enc_forward_hidden_layer\n",
    "            for i in range(max_tar_len):\n",
    "                curr_target = tar[i]\n",
    "                curr_mask = i < tar_lens\n",
    "                # dec_hidden assign to self to for next step\n",
    "                dec_pred, dec_hidden = decoder.forward(dec_hidden, dec_input, enc_output)\n",
    "                # dec_pred # B x vocab_size_of_translated_lang\n",
    "                # assign previous output as input\n",
    "                dec_input = dec_pred.argmax(dim=1)\n",
    "                # loss calculation\n",
    "                curr_avgloss, curr_non_zero = loss(curr_target,dec_pred,curr_mask)\n",
    "                total_avgloss += curr_avgloss\n",
    "                total_count += curr_non_zero.item()\n",
    "                total_loss += curr_avgloss.item() * curr_non_zero.item() \n",
    "        \n",
    "        # backpropagate loss\n",
    "        total_avgloss.backward()\n",
    "        \n",
    "        # clip the gradients # save from exploring gradient\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "        \n",
    "        # optimizer perform the step # Does the update\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class beam_candidate:\n",
    "    def __init__(self):\n",
    "        # candidate's backtacking history\n",
    "        self.output_word_index_till_now = []\n",
    "        # log probability to avoid underflow => a*b = log(a)+log(b)\n",
    "        self.log_prob_till_now = 0 \n",
    "        # required for next state decoder RNN\n",
    "        self.latest_hidden_state_generated = None\n",
    "        # https://www.coursera.org/lecture/nlp-sequence-models/refinements-to-beam-search-AkjG2\n",
    "        self.length_normalized_res = 0.0\n",
    "        self.last_word_seen = 1 # <SOS> (this can also get from above array last element)\n",
    "        \n",
    "    def add_element(self,word_index,prob,latest_hidden_state):\n",
    "        self.output_word_index_till_now.append(word_index)\n",
    "        self.log_prob_till_now += np.log(prob)\n",
    "        self.latest_hidden_state_generated = latest_hidden_state\n",
    "        self.length_normalized_res = 1.0 / (len(self.output_word_index_till_now)**0.7) * self.log_prob_till_now\n",
    "        self.last_word_seen = word_index\n",
    "        return self\n",
    "    \n",
    "    def deepcopy(self):\n",
    "        b = beam_candidate()\n",
    "        b.output_word_index_till_now = self.output_word_index_till_now[:]\n",
    "        b.log_prob_till_now = self.log_prob_till_now\n",
    "        b.latest_hidden_state_generated = self.latest_hidden_state_generated\n",
    "        b.length_normalized_res = self.length_normalized_res\n",
    "        b.last_word_seen = self.last_word_seen\n",
    "        return b\n",
    "    \n",
    "    # debug purpose printing the results !\n",
    "    def __str__(self):\n",
    "        return \"output_word_index_till_now : \" + str(self.output_word_index_till_now) + \" length_normalized_res \" + str(self.length_normalized_res)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"output_word_index_till_now : \" + str(self.output_word_index_till_now) + \" length_normalized_res \" + str(self.length_normalized_res)\n",
    "        \n",
    "def beam_search(enc,dec,beam_width,input_seq, num_layers, num_directions,hidden_size, max_lenght=30):\n",
    "    \"\"\"\n",
    "    # this beam search is not based on batch !\n",
    "    input = word_indexes tensor\n",
    "    \"\"\"\n",
    "    # beam search for validation and testing !\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    \n",
    "    # convert input to tensors !\n",
    "    src = torch.LongTensor(input_seq)\n",
    "    src = src.view((-1,1)) # T x B x words \n",
    "    src_len = torch.LongTensor([len(input_seq),]) # 1 x lenght # dummy batched version\n",
    "    \n",
    "    src = src.to(device)\n",
    "    src_lens = src_len.to(device)\n",
    "    \n",
    "    enc_output, enc_hidden = enc.forward(src,src_lens)\n",
    "    enc_hidden = enc_hidden.view(num_layers, num_directions, 1, hidden_size) # batch = 1 , dummy batch\n",
    "    enc_forward_hidden_layer = enc_hidden[:,0]\n",
    "    enc_forward_hidden_layer = enc_forward_hidden_layer.contiguous()\n",
    "    \n",
    "    dec_input = torch.LongTensor([1,]) # dummy batch # <SOS> as first character !\n",
    "    dec_input = dec_input.to(device)\n",
    "    \n",
    "    start_candidate = beam_candidate().add_element(1,1,enc_forward_hidden_layer)\n",
    "    \n",
    "    # beam search start !\n",
    "    candidates = [start_candidate]\n",
    "    next_round_candidates = []\n",
    "    results = []\n",
    "    for c in range(max_lenght):\n",
    "        if len(results) >= beam_width:\n",
    "            break\n",
    "        \n",
    "        if len(candidates)==0:\n",
    "            break\n",
    "        for each in candidates:\n",
    "            # pass through decoder\n",
    "            last_word = torch.LongTensor([each.last_word_seen,]).to(device) # dummy batched tensor\n",
    "            dec_pred, dec_hidden = dec.forward(each.latest_hidden_state_generated,last_word,\n",
    "                        enc_output)\n",
    "            dec_pred = dec_pred.squeeze(0) # dummy batch removed\n",
    "            topk_prob, topk_indexes = torch.topk(dec_pred,beam_width)\n",
    "            for each_prob, each_index in zip(topk_prob,topk_indexes):\n",
    "                each_next = each.deepcopy()\n",
    "                each_next.add_element(each_index.item(), each_prob.item(), dec_hidden)\n",
    "                next_round_candidates.append(each_next)\n",
    "        \n",
    "        # sort the candidates and pick top B candidates (B=beam_width)\n",
    "        next_round_candidates.sort(key=lambda x: x.length_normalized_res, reverse=True)\n",
    "        # before picking top beam width candidates\n",
    "        # select candidates with <EOS> and put in the results \n",
    "        # else add them for further search\n",
    "        next_round_candidates_top_b = []\n",
    "        for each_cand in next_round_candidates:\n",
    "            # take top B candidates only (B=beam width)\n",
    "            if len(next_round_candidates_top_b) >= beam_width:\n",
    "                break\n",
    "                \n",
    "            if each_cand.last_word_seen == 2 or c==max_lenght-1: # check for <EOS> or last round \n",
    "                results.append(each_cand)\n",
    "            else: \n",
    "                next_round_candidates_top_b.append(each_cand)\n",
    "                \n",
    "        candidates = next_round_candidates_top_b[:]\n",
    "        next_round_candidates = []\n",
    "        \n",
    "    # final candidates ... top picks !\n",
    "    results.sort(key=lambda x: x.length_normalized_res, reverse=True)\n",
    "    best_candidate_words = results[0].output_word_index_till_now\n",
    "    # remove <SOS> and <EOS> from best output !\n",
    "    if 1 in best_candidate_words:\n",
    "        best_candidate_words.remove(1)\n",
    "    if 2 in best_candidate_words:\n",
    "        best_candidate_words.remove(2)\n",
    "    return best_candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(enc,dec,beam_width,num_layers,num_directions,hidden_size):\n",
    "    \"\"\"\n",
    "    returns a blue score for test dataset\n",
    "    \"\"\"\n",
    "    batch = 1\n",
    "    number_iter = len(val_splitted_sentence_pair)\n",
    "    val_dataset = Dataset(val_splitted_sentence_pair,english,german,batch_size=1)\n",
    "    blue_scores = []\n",
    "    # dataset get_batch having infinite loop ... get limited loop and iterate once over data\n",
    "    for i in range(number_iter):\n",
    "        src, tar, src_lens, tar_lens = val_dataset.get_batch()\n",
    "        # target batch size is 1 # remove <EOS> tag !\n",
    "        if 2 in tar[0]:\n",
    "            tar[0].remove(2)\n",
    "        res = beam_search(enc,dec,beam_width=beam_width,input_seq=src[0],num_layers=num_layers, \n",
    "                          num_directions=num_directions,hidden_size=hidden_size)\n",
    "        score = sentence_bleu(tar, res,smoothing_function=chencherry.method2)\n",
    "        blue_scores.append(score)\n",
    "    return np.mean(blue_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state,file_name):\n",
    "    torch.save(state, file_name)\n",
    "    \n",
    "def load_checkpoint(file_name,enc_model,dec_model,enc_optimizer,dec_optimizer):\n",
    "    epoch, best_acc = 0 , 0.0\n",
    "    if os.path.isfile(file_name):\n",
    "        print(\"=> loading checkpoint '{}'\".format(file_name))\n",
    "        checkpoint = torch.load(file_name)\n",
    "        \n",
    "        epoch = int(checkpoint['epoch'])\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        \n",
    "        enc_model.load_state_dict(checkpoint['enc_state_dict'])\n",
    "        dec_model.load_state_dict(checkpoint['dec_state_dict'])\n",
    "        enc_optimizer.load_state_dict(checkpoint['enc_optimizer'])\n",
    "        dec_optimizer.load_state_dict(checkpoint['dec_optimizer'])\n",
    "        \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(file_name))\n",
    "    \n",
    "    return epoch, best_acc, enc_model, dec_model, enc_optimizer, dec_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    learning_rate = 0.0001\n",
    "    clip = 50.0\n",
    "    teacher_forcing_ratio = 0.75\n",
    "    num_layers, num_directions, batch, hidden_size = 2, 2, 32, 500\n",
    "    beam_width = 30\n",
    "    num_epochs = 50000\n",
    "    print_loss_every_iter = 100\n",
    "    val_every_iter = 100\n",
    "    save_model_every_iter = 1000\n",
    "    file_name = \"/gpu_data/pkabara/data/translation/seq2seq\"\n",
    "    dataset = Dataset(train_splitted_sentence_pair,english,german,batch_size=batch)\n",
    "    \n",
    "    enc = Encoder(len(english.word2index),hidden_size,num_layers=num_layers)\n",
    "    dec = Decoder(len(german.word2index),hidden_size)\n",
    "    enc_optimizer = optim.Adam(enc.parameters(), lr=learning_rate)\n",
    "    dec_optimizer = optim.Adam(dec.parameters(), lr=learning_rate)\n",
    "    \n",
    "    epoch, best_acc, enc, dec, enc_optimizer, dec_optimizer = load_checkpoint(file_name,enc,dec,enc_optimizer,dec_optimizer)\n",
    "    \n",
    "    enc = enc.to(device)\n",
    "    dec = dec.to(device)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "    \n",
    "    for i in range(epoch,epoch+num_epochs):\n",
    "        src, tar, src_lens, tar_lens = dataset.get_batch()\n",
    "        max_tar_len = max(tar_lens)\n",
    "        \n",
    "        batch_loss = train_a_batch(enc,dec,src,src_lens,tar,tar_lens,\n",
    "                  max_tar_len,\n",
    "                  enc_optimizer,dec_optimizer,\n",
    "                  num_layers, num_directions, batch, hidden_size,\n",
    "                  teacher_forcing_ratio=teacher_forcing_ratio,clip=clip)\n",
    "        \n",
    "        running_loss += batch_loss\n",
    "        running_count += 1\n",
    "        \n",
    "        if running_count % print_loss_every_iter == 0:\n",
    "            print (\"Iteration {} Running loss {}\".format(running_count, running_loss/running_count))\n",
    "        \n",
    "        if running_count % val_every_iter == 0:\n",
    "            score = validation(enc,dec,beam_width,num_layers,num_directions,hidden_size)\n",
    "            print (\"Iteration {} Validation BLUE {}\".format(running_count, score))\n",
    "            if score > best_acc:\n",
    "                best_acc = score\n",
    "        \n",
    "                # save every some iterations ... if running_count % save_model_every_iter == 0:\n",
    "                state = {\n",
    "                    'epoch': epoch+1,\n",
    "                    'best_acc': best_acc,\n",
    "                    'enc_state_dict': enc.state_dict(),\n",
    "                    'dec_state_dict': dec.state_dict(),\n",
    "                    'enc_optimizer' : enc_optimizer.state_dict(),\n",
    "                    'dec_optimizer' : dec_optimizer.state_dict()\n",
    "                }\n",
    "                save_checkpoint(state,file_name)\n",
    "                print (\"saved checkpoint !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> no checkpoint found at '/gpu_data/pkabara/data/translation/seq2seq'\n",
      "Iteration 100 Running loss nan\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCStorage.cpp:36",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-0a196eec38c6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrunning_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mval_every_iter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_directions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration {} Validation BLUE {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-0c290480c041>\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(enc, dec, beam_width, num_layers, num_directions, hidden_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         res = beam_search(enc,dec,beam_width=beam_width,input_seq=src[0],num_layers=num_layers, \n\u001b[0;32m---> 16\u001b[0;31m                           num_directions=num_directions,hidden_size=hidden_size)\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmoothing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchencherry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mblue_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-38bf71a8c28b>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(enc, dec, beam_width, input_seq, num_layers, num_directions, hidden_size, max_lenght)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0meach_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopk_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopk_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0meach_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0meach_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mnext_round_candidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCStorage.cpp:36"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
